import os

import cv2

import random

import warnings

import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, cohen_kappa_score

from keras.models import Model

from keras import optimizers, applications

from keras.preprocessing.image import ImageDataGenerator

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input



# Set seeds to make the experiment more reproducible.

from tensorflow import set_random_seed

def seed_everything(seed=0):

    random.seed(seed)

    os.environ['PYTHONHASHSEED'] = str(seed)

    np.random.seed(seed)

    set_random_seed(0)

seed_everything()




sns.set(style="whitegrid")

warnings.filterwarnings("ignore")
train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')

test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')
sns.set_style("white")

count = 1

plt.figure(figsize=[20, 20])

for img_name in train['id_code'][:15]:

    img = cv2.imread("../input/aptos2019-blindness-detection/train_images/%s.png" % img_name)[...,[2, 1, 0]]

    plt.subplot(5, 5, count)

    plt.imshow(img)

    plt.title("Image %s" % count)

    count += 1

    

plt.show()
# Model parameters

BATCH_SIZE = 8

EPOCHS = 20

WARMUP_EPOCHS = 2

LEARNING_RATE = 1e-4

WARMUP_LEARNING_RATE = 1e-3

HEIGHT = 512

WIDTH = 512

CANAL = 3

N_CLASSES = train['diagnosis'].nunique()

ES_PATIENCE = 5

RLROP_PATIENCE = 3

DECAY_DROP = 0.5
# Preprocecss data

train["id_code"] = train["id_code"].apply(lambda x: x + ".png")

test["id_code"] = test["id_code"].apply(lambda x: x + ".png")

train['diagnosis'] = train['diagnosis'].astype('str')

train.head()
train_datagen=ImageDataGenerator(rescale=1./255, 

                                 validation_split=0.2,

                                 horizontal_flip=True)



train_generator=train_datagen.flow_from_dataframe(

    dataframe=train,

    directory="../input/aptos2019-blindness-detection/train_images/",

    x_col="id_code",

    y_col="diagnosis",

    batch_size=BATCH_SIZE,

    class_mode="categorical",

    target_size=(HEIGHT, WIDTH),

    subset='training')



valid_generator=train_datagen.flow_from_dataframe(

    dataframe=train,

    directory="../input/aptos2019-blindness-detection/train_images/",

    x_col="id_code",

    y_col="diagnosis",

    batch_size=BATCH_SIZE,

    class_mode="categorical",    

    target_size=(HEIGHT, WIDTH),

    subset='validation')



test_datagen = ImageDataGenerator(rescale=1./255)



test_generator = test_datagen.flow_from_dataframe(  

        dataframe=test,

        directory = "../input/aptos2019-blindness-detection/test_images/",

        x_col="id_code",

        target_size=(HEIGHT, WIDTH),

        batch_size=1,

        shuffle=False,

        class_mode=None)
def create_model(input_shape, n_out):

    input_tensor = Input(shape=input_shape)

    base_model = applications.ResNet50(weights=None, 

                                       include_top=False,

                                       input_tensor=input_tensor)

    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')



    x = GlobalAveragePooling2D()(base_model.output)

    x = Dropout(0.5)(x)

    x = Dense(2048, activation='relu')(x)

    x = Dropout(0.5)(x)

    final_output = Dense(n_out, activation='softmax', name='final_output')(x)

    model = Model(input_tensor, final_output)

    

    return model
model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)



for layer in model.layers:

    layer.trainable = False



for i in range(-5, 0):

    model.layers[i].trainable = True



metric_list = ["accuracy"]

optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)

model.compile(optimizer=optimizer, loss="categorical_crossentropy",  metrics=metric_list)

keras.callbacks.ModelCheckpoint('', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)

model.summary()
STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size

STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size



history_warmup = model.fit_generator(generator=train_generator,

                              steps_per_epoch=STEP_SIZE_TRAIN,

                              validation_data=valid_generator,

                              validation_steps=STEP_SIZE_VALID,

                              epochs=WARMUP_EPOCHS,

                              verbose=1).history
for layer in model.layers:

    layer.trainable = True



es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)

rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)

checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)

callback_list = [es, rlrop]

optimizer = optimizers.Adam(lr=LEARNING_RATE)

model.compile(optimizer=optimizer, loss="binary_crossentropy",  metrics=metric_list)

model.summary()

model.save("model.h5")
history_finetunning = model.fit_generator(generator=train_generator,

                              steps_per_epoch=STEP_SIZE_TRAIN,

                              validation_data=valid_generator,

                              validation_steps=STEP_SIZE_VALID,

                              epochs=EPOCHS,

                              callbacks=callback_list,

                              verbose=1).history
history = {'loss': history_warmup['loss'] + history_finetunning['loss'], 

           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'], 

           'acc': history_warmup['acc'] + history_finetunning['acc'], 

           'val_acc': history_warmup['val_acc'] + history_finetunning['val_acc']}



sns.set_style("whitegrid")

fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))



ax1.plot(history['loss'], label='Train loss')

ax1.plot(history['val_loss'], label='Validation loss')

ax1.legend(loc='best')

ax1.set_title('Loss')



ax2.plot(history['acc'], label='Train Accuracy')

ax2.plot(history['val_acc'], label='Validation accuracy')

ax2.legend(loc='best')

ax2.set_title('Accuracy')



plt.xlabel('Epochs')

sns.despine()

plt.show()
complete_datagen = ImageDataGenerator(rescale=1./255)

complete_generator = complete_datagen.flow_from_dataframe(  

        dataframe=train,

        directory = "../input/aptos2019-blindness-detection/train_images/",

        x_col="id_code",

        target_size=(HEIGHT, WIDTH),

        batch_size=1,

        shuffle=False,

        class_mode=None)



STEP_SIZE_COMPLETE = complete_generator.n//complete_generator.batch_size

train_preds = model.predict_generator(complete_generator, steps=STEP_SIZE_COMPLETE)

train_preds = [np.argmax(pred) for pred in train_preds]
labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']

cnf_matrix = confusion_matrix(train['diagnosis'].astype('int'), train_preds)

cnf_matrix_norm = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]

df_cm = pd.DataFrame(cnf_matrix_norm, index=labels, columns=labels)

plt.figure(figsize=(16, 7))

sns.heatmap(df_cm, annot=True, fmt='.2f', cmap="Blues")

plt.show()
print("Train Cohen Kappa score: %.3f" % cohen_kappa_score(train_preds, train['diagnosis'].astype('int'), weights='quadratic'))
test_generator.reset()

STEP_SIZE_TEST = test_generator.n//test_generator.batch_size

preds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)

predictions = [np.argmax(pred) for pred in preds]
filenames = test_generator.filenames

results = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})

results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])

results.to_csv('submission.csv',index=False)

results.head(10)
f, ax = plt.subplots(figsize=(14, 8.7))

ax = sns.countplot(x="diagnosis", data=results, palette="GnBu_d")

sns.despine()

plt.show()