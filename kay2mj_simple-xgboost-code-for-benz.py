import pandas as pd

import numpy as np

import xgboost as xgb

np.set_printoptions(threshold=np.inf)



# data for train set

df = pd.read_csv("../input/train.csv")
# data transform (cycle =26)

df = df.replace(["a","b","c","d","e","f","g","h", \

"i","j","k","l","m","n","o","p","q","r","s","t",  \

"u","v","w","x","y","z","aa","ab","ac","ad","ae",  \

"af","ag","ah","ai","aj","ak","al","am","an","ao","ap", \

"aq","ar","as","at","au","av","aw","ax","ay","az", \

"ba", "bb", "bc", "bd"], \

[0.038461538461538464, 0.07692307692307693, 0.11538461538461539,\

0.15384615384615385, 0.19230769230769232, 0.23076923076923078, \

0.2692307692307692, 0.3076923076923077, 0.34615384615384615,\

0.38461538461538464,0.4230769230769231, 0.46153846153846156, \

0.5, 0.5384615384615384, 0.5769230769230769, 0.6153846153846154, \

0.6538461538461539, 0.6923076923076923, 0.7307692307692307, \

0.7692307692307693, 0.8076923076923077, 0.8461538461538461, \

0.8846153846153846, 0.9230769230769231, 0.9615384615384616, \

1, 1.0384615384615385, 1.0769230769230769, 1.1153846153846154, \

1.1538461538461537, 1.1923076923076923, 1.2307692307692308, \

1.2692307692307692, 1.3076923076923077, 1.3461538461538463, \

1.3846153846153846,1.4230769230769231, 1.4615384615384615, \

1.5,1.5384615384615385, 1.5769230769230769, 1.6153846153846154, \

1.6538461538461537, 1.6923076923076923, 1.7307692307692308, \

1.7692307692307692, 1.8076923076923077, 1.8461538461538463, \

1.8846153846153846, 1.9230769230769231, 1.9615384615384615, \

2.0, 2.0384615384615383, 2.076923076923077, 2.1153846153846154, \

2.1538461538461537])
y = df[["y"]]

X_data = df.iloc[:, 2:]

X_id = df[["ID"]]
X = pd.concat([X_id, X_data], axis =1)
# data split

from sklearn.cross_validation import train_test_split



X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2)

# model (you can change the parameters)

from xgboost import XGBRegressor

model = XGBRegressor(base_score=0.5, colsample_bylevel=0.7, colsample_bytree=1, gamma=5,

       learning_rate=0.1, max_delta_step=0, max_depth=2,

       min_child_weight=1, missing=0.7, n_estimators=80, nthread=-1,

       objective='reg:linear', reg_alpha=0.6, reg_lambda=1,

       scale_pos_weight=1, seed=5, silent=True, subsample=1)
model.fit(X, y)

model.score(X_test, y_test)

# test data

df1 = pd.read_csv("../input/test.csv")
df1 = df1.replace(["a","b","c","d","e","f","g","h", \

"i","j","k","l","m","n","o","p","q","r","s","t",  \

"u","v","w","x","y","z","aa","ab","ac","ad","ae",  \

"af","ag","ah","ai","aj","ak","al","am","an","ao","ap", \

"aq","ar","as","at","au","av","aw","ax","ay","az", \

"ba", "bb", "bc", "bd"], \

[0.038461538461538464, 0.07692307692307693, 0.11538461538461539,\

0.15384615384615385, 0.19230769230769232, 0.23076923076923078, \

0.2692307692307692, 0.3076923076923077, 0.34615384615384615,\

0.38461538461538464,0.4230769230769231, 0.46153846153846156, \

0.5, 0.5384615384615384, 0.5769230769230769, 0.6153846153846154, \

0.6538461538461539, 0.6923076923076923, 0.7307692307692307, \

0.7692307692307693, 0.8076923076923077, 0.8461538461538461, \

0.8846153846153846, 0.9230769230769231, 0.9615384615384616, \

1, 1.0384615384615385, 1.0769230769230769, 1.1153846153846154, \

1.1538461538461537, 1.1923076923076923, 1.2307692307692308, \

1.2692307692307692, 1.3076923076923077, 1.3461538461538463, \

1.3846153846153846,1.4230769230769231, 1.4615384615384615, \

1.5,1.5384615384615385, 1.5769230769230769, 1.6153846153846154, \

1.6538461538461537, 1.6923076923076923, 1.7307692307692308, \

1.7692307692307692, 1.8076923076923077, 1.8461538461538463, \

1.8846153846153846, 1.9230769230769231, 1.9615384615384615, \

2.0, 2.0384615384615383, 2.076923076923077, 2.1153846153846154, \

2.1538461538461537])

           
X_data_test = df1.iloc[:, 1:]

X_id_test = df1[["ID"]]
X_Test = pd.concat([X_id_test, X_data_test], axis =1)
# Bagging Regressor

y_pred = model.predict(X_Test)
# DataFrameに変換

y_pred = pd.DataFrame(y_pred)
result = pd.concat([df1[["ID"]], y_pred], axis = 1)
result.to_csv("result.csv", index=False)
